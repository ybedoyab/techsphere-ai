#!/usr/bin/env python3
"""
üè• Challenge de Clasificaci√≥n Biom√©dica con IA
Script principal usando arquitectura limpia y patrones de dise√±o
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, Any

# Agregar src al path
sys.path.append(str(Path(__file__).parent / "src"))

from src.config.settings import config
from src.utils.logger import main_logger
from src.data.repository import CSVDataSource, DataRepository, DataService
from src.features.text_preprocessor import TextPreprocessingPipeline, FeatureExtractor


def main():
    """
    üöÄ Funci√≥n principal del pipeline
    """
    parser = argparse.ArgumentParser(
        description="üè• An√°lisis del Dataset Biom√©dico con IA",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python main.py                    # An√°lisis completo
  python main.py --export-v0        # Exportar para V0
  python main.py --preprocess       # Solo preprocesamiento
  python main.py --help             # Ver ayuda
        """
    )
    
    parser.add_argument(
        '--export-v0',
        action='store_true',
        help='Exportar an√°lisis para visualizaci√≥n V0'
    )
    
    parser.add_argument(
        '--preprocess',
        action='store_true',
        help='Ejecutar solo preprocesamiento de texto'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Modo verbose con m√°s informaci√≥n'
    )
    
    args = parser.parse_args()
    
    try:
        main_logger.info("üöÄ INICIANDO AN√ÅLISIS BIOM√âDICO")
        main_logger.info("=" * 60)
        
        # Mostrar configuraci√≥n
        if args.verbose:
            main_logger.info("‚öôÔ∏è Configuraci√≥n del proyecto:")
            config_dict = config.to_dict()
            for key, value in config_dict.items():
                main_logger.info(f"  {key}: {value}")
        
        # 1. Cargar datos
        main_logger.info("\nüì• PASO 1: CARGANDO DATOS")
        main_logger.info("-" * 40)
        
        data_source = CSVDataSource(
            file_path=config.data.raw_data_path,
            separator=config.data.csv_separator
        )
        
        repository = DataRepository(data_source)
        data_service = DataService(repository)
        
        # Informaci√≥n b√°sica
        data_info = repository.get_data_info()
        main_logger.info(f"üìä Informaci√≥n del dataset:")
        main_logger.info(f"  ‚Ä¢ Forma: {data_info['shape']}")
        main_logger.info(f"  ‚Ä¢ Columnas: {data_info['columns']}")
        main_logger.info(f"  ‚Ä¢ Uso de memoria: {data_info['memory_usage'] / 1024 / 1024:.2f} MB")
        
        # 2. An√°lisis exploratorio
        main_logger.info("\nüîç PASO 2: AN√ÅLISIS EXPLORATORIO")
        main_logger.info("-" * 40)
        
        analysis = data_service.analyze_dataset()
        
        # Mostrar resultados del an√°lisis
        _display_analysis_results(analysis)
        
        # 3. Preprocesamiento de texto
        if args.preprocess or args.export_v0:
            main_logger.info("\nüîÑ PASO 3: PREPROCESAMIENTO DE TEXTO")
            main_logger.info("-" * 40)
            
            # Cargar datos
            df = repository.load_data()
            
            # Preprocesar texto
            text_pipeline = TextPreprocessingPipeline()
            processed_df = text_pipeline.preprocess_dataframe(
                df, 
                text_columns=['title', 'abstract']
            )
            
            main_logger.info("‚úÖ Preprocesamiento de texto completado")
            
            # Guardar datos preprocesados
            processed_path = config.data.processed_data_path + "processed_data.csv"
            repository.save_data(processed_df, processed_path)
            
            # 4. Extracci√≥n de caracter√≠sticas
            main_logger.info("\nüîç PASO 4: EXTRACCI√ìN DE CARACTER√çSTICAS")
            main_logger.info("-" * 40)
            
            feature_extractor = FeatureExtractor()
            
            # Combinar t√≠tulo y abstract procesados
            combined_texts = (
                processed_df['title_processed'] + ' ' + 
                processed_df['abstract_processed']
            ).fillna('')
            
            # Extraer caracter√≠sticas TF-IDF
            text_features = feature_extractor.extract_text_features(
                combined_texts.tolist(),
                max_features=config.text.max_features
            )
            
            # Extraer metadatos
            metadata_features = feature_extractor.extract_metadata_features(processed_df)
            
            # Codificar etiquetas
            encoded_labels = feature_extractor.encode_labels(processed_df['group'].tolist())
            
            main_logger.info("‚úÖ Extracci√≥n de caracter√≠sticas completada")
            main_logger.info(f"  ‚Ä¢ Caracter√≠sticas TF-IDF: {text_features.shape}")
            main_logger.info(f"  ‚Ä¢ Metadatos: {metadata_features.shape}")
            main_logger.info(f"  ‚Ä¢ Etiquetas codificadas: {encoded_labels.shape}")
            
            # Guardar caracter√≠sticas
            import numpy as np
            features_path = config.data.processed_data_path + "features.npz"
            np.savez_compressed(
                features_path,
                text_features=text_features,
                metadata_features=metadata_features,
                labels=encoded_labels
            )
            
            main_logger.info(f"üíæ Caracter√≠sticas guardadas en: {features_path}")
        
        # 5. Exportar para V0
        if args.export_v0:
            main_logger.info("\nüéØ PASO 5: EXPORTANDO PARA V0")
            main_logger.info("-" * 40)
            
            success = data_service.export_for_v0()
            
            if success:
                main_logger.info("‚úÖ Datos exportados exitosamente para V0")
                main_logger.info("üé® Archivo listo para visualizaciones avanzadas")
            else:
                main_logger.error("‚ùå Error al exportar para V0")
        
        # Resumen final
        main_logger.info("\nüéâ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        main_logger.info("=" * 60)
        main_logger.info("üìÅ Archivos generados:")
        main_logger.info(f"  ‚Ä¢ Datos preprocesados: {config.data.processed_data_path}")
        main_logger.info(f"  ‚Ä¢ Caracter√≠sticas: {config.data.processed_data_path}features.npz")
        main_logger.info(f"  ‚Ä¢ An√°lisis V0: results/v0_analysis_data.json")
        main_logger.info(f"  ‚Ä¢ Logs: {config.logs_dir}")
        
        main_logger.info("\nüöÄ Pr√≥ximos pasos:")
        main_logger.info("  1. Revisar resultados en 'results/v0_analysis_data.json'")
        main_logger.info("  2. Usar V0 para visualizaciones avanzadas")
        main_logger.info("  3. Entrenar modelos de ML con las caracter√≠sticas extra√≠das")
        
    except Exception as e:
        main_logger.error(f"‚ùå ERROR CR√çTICO: {e}")
        main_logger.error("üîç Revisar logs para m√°s detalles")
        sys.exit(1)


def _display_analysis_results(analysis: Dict[str, Any]):
    """
    üìä Mostrar resultados del an√°lisis de forma organizada
    """
    # Informaci√≥n b√°sica
    basic_info = analysis['basic_info']
    main_logger.info(f"üìä Informaci√≥n b√°sica:")
    main_logger.info(f"  ‚Ä¢ Total de registros: {basic_info['total_records']:,}")
    main_logger.info(f"  ‚Ä¢ Columnas: {basic_info['columns']}")
    
    # An√°lisis de etiquetas
    label_analysis = analysis['label_analysis']
    main_logger.info(f"\nüè∑Ô∏è An√°lisis de etiquetas:")
    main_logger.info(f"  ‚Ä¢ Etiquetas √∫nicas: {label_analysis['unique_labels']}")
    main_logger.info(f"  ‚Ä¢ Single-label: {label_analysis['single_label_count']:,} ({label_analysis['single_label_percentage']:.1f}%)")
    main_logger.info(f"  ‚Ä¢ Multi-label: {label_analysis['multi_label_count']:,} ({label_analysis['multi_label_percentage']:.1f}%)")
    
    # Distribuci√≥n por dominio
    domain_distribution = analysis['domain_distribution']
    main_logger.info(f"\nüè• Distribuci√≥n por dominio:")
    for domain, stats in domain_distribution.items():
        main_logger.info(f"  ‚Ä¢ {domain}: {stats['count']:,} ({stats['percentage']:.1f}%)")
    
    # Estad√≠sticas de texto
    text_stats = analysis['text_statistics']
    main_logger.info(f"\nüìù Estad√≠sticas de texto:")
    main_logger.info(f"  ‚Ä¢ T√≠tulos - Promedio: {text_stats['avg_title_length']:.1f} caracteres")
    main_logger.info(f"  ‚Ä¢ Abstracts - Promedio: {text_stats['avg_abstract_length']:.1f} caracteres")
    
    # Calidad de datos
    data_quality = analysis['data_quality']
    main_logger.info(f"\nüîç Calidad de datos:")
    main_logger.info(f"  ‚Ä¢ Valores faltantes: {sum(data_quality['missing_values'].values())}")
    main_logger.info(f"  ‚Ä¢ Duplicados: {data_quality['duplicates']}")
    main_logger.info(f"  ‚Ä¢ Cadenas vac√≠as: {sum(data_quality['empty_strings'].values())}")


if __name__ == "__main__":
    main()
